# Deep Learning From Scratch
Project to implement a Feed Forward Neural Network from scratch (from the pseudo-code). It implements a backpropagation algorithm 
and a greedy layer-wise training of the neural network.
It starts from the implementation of an RBM (Restricted Boltzmann Machine) which can be trained through the contrastive divergence 
algorithm. The RBM serves as an elementary brick to build a DBN (Deep Belief Network). The latter structure is useful to run the 
greedy layer-wise step training to initialize the parameters of the neural network. Finally, the DNN (Deep Neural Network) uses the
same architecture as the DBN to run the backpropagation algorithm to learn the parameters of the network.

# Getting started
## Prerequisites
You need Python, Scipy and Numpy installed on your computer to be able to run the files. You can run them either from the terminal 
or from an IDE like Spyder.

## Running the code
You can choose to run either an RBM, a DBN or a DNN. Either one of this structure can be initialized through this code, trained and
generate data in the case of the RBM and the DBN or classify data in the case of the DNN. Every training algorithm relies on a 
gradient descent algorithm, which can be run by mini-batch.
You can run the RBM code by importing it into your script as follows:
```
import RBM
import DBN
```

The code can be used to fully build an train a feed forward neural network. It goes as follows:

* The DNN is initialized with a number of layers and of neurons per layers. Each layer has its own weights and biases. The weights 
are initialized randomly following a normaldistribution of mean 0 and standard deviation 0.1 (which can be changed in the code). 
The biases are put at 0 for each layer. To initialize a network, simply run the following on your IDE:
```
list_layers = [100, 200, 200, 10] #list of neurons per layers, edfines the architecture of the network
dbn = DBN.DBN(list_layers) #create the instance 'dbn' of the class 'DBN'
```

* The network is trained through greedy layer-wise procedure with the whole dataset. This algorithm relies on the contrastive 
divergence algorithm. A greedy layer-wise training allows one to initialize the parameters of the neural network before training it
 through a backpropagation. Note that this step can be skipped and is usually skipped nowadays because of the quantity of data 
 available to train networks. To train through the greedy layer-wise procedure.
 ```
 dbn.train_DBN(data, nb_iter_grad, learning_rate=0.1, batch_size=5)
 ```
 
* Finally we can train the network with backpropagation using only the train dataset this time. The 'entropies' variable generated by the 
function is a list containing the value of the entropy at each iteration of the gradient descent algorithm, to see that it indeed decreases
with the number of iterations. To train the network through backpropagation, do:
 ```
 entropies = dbn.backpropagation(data=train_data, labels=train_labels, nb_iter_grad=100, learning_rate=0.1, batch_size=5)
 ```
 
 To initialize and train an RBM, simply do:
 ```
 rbm = RBM.RBM(visible_size, hidden_size) #instantiate an RBM with the size of its visible layer and the size of its hidden layer.
 MSE = rbm.train_RBM(data, nb_iter=100, batch_size=5, learning_rate=0.1)
 ```
 The 'MSE' variable corresponds to the mean squared error of the RBM at the end of its training.

 To generate data with the RBM, do:
 ```
 v, h = rbm.generate_image(nb_gibbs_iter) #nb_gibbs_iter is the number of steps in the Gibbs sampling algorithm, used to generate data
 ```
 v and h are the visible and hidden data sampled at the end of the Gibbs sampling algorithm. It can be interesting to observe the visible 
 data generated after training the RBM on images for example.
 
 To generate data with the DBN, do:
```
v = dbn.generate_image(nb_gibbs_iter)
```
This time, only data on the visible layer (first layer, where data comes in) is generated. You might as well want to test out this function 
on images.

Finally, you can test out the DNN's classification performance by doing:
```
rate = dbn.test_DNN(data, labels)
```
The 'rate' variable is the rate of good classification of the data in input (well classified data per number of data).
